from typing import Tuple
import torch
from torch import nn, Tensor

from _context import sparse
from sparse import util

from spalbp.attention.config import AdaptiveSparseAttentionConfig


class _OneDimensionalSparseAttention(nn.Module):

    def __init__(self,
                 emb: int,
                 n_heads: int,
                 *,
                 k: int = 4,
                 gadditional: int = 1,
                 nadditional: int = 4):
        super().__init__()
        self.emb = emb
        self.n_heads = n_heads
        self.k = k
        self.gadditional = gadditional
        self.nadditional = nadditional

        self.to_keys = nn.Linear(emb, emb * n_heads, bias=False)
        self.to_queries = nn.Linear(emb, emb * n_heads, bias=False)
        self.to_values = nn.Linear(emb, emb * n_heads, bias=False)
        self.unify = nn.Linear(emb * n_heads, emb)

        self.register_buffer('mvalues', torch.ones((k,)))

    def hyper(self, x: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        raise NotImplementedError("You must implement this yourself")

    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        means, sigmas, values = self.hyper(x)  # (B, C, k, 1); (B, C, k, 1); (B, C, k)
        batch, context, emb = x.size()  # (B, C, E)
        rank = means.size(-1)
        indices: Tensor = sparse.ngenerate(means,
                                           self.gadditional if self.training else 0,  # For evaluation, only get nearest
                                           self.nadditional if self.training else 0,  # index for each point
                                           rng=(context,),
                                           relative_range=(3,),
                                           cuda='cuda' in util.d(x))  # (B, C, P, 1)
        assert ((indices < 0).sum().item() == 0) and ((indices >= context).sum().item() == 0), \
            f'Found some indices out of bounds: indices < 0: {(indices < 0).sum().item()}; ' \
            f'indices >= {context}: {(indices >= context).sum().item()}'
        indices_fl = indices.float()
        # For each point (self.k), we expect to sample the 2**rank closest points from the first set of sampling,
        # then self.gadditional globally-sampled indices, and self.nadditional neighborhood-sampled indices.
        num_points = self.k * (2 ** rank + ((self.gadditional + self.nadditional) if self.training else 0))
        assert indices.size() == (batch, context, num_points, 1), f'Expected size {(batch, context, num_points, 1)}. ' \
                                                                  f'Got {indices.size()}'
        densities = sparse.densities(indices_fl, means, sigmas).clone()  # (B, C, P, self.k)
        duplicates = util.nduplicates(indices).to(torch.bool)  # (B, C, P) boolean mask of duplicates all-but-one
        densities[duplicates, :] = 0  # Removes all duplicates

        # Normalize densities across all K probability distributions by summing
        densities = densities / densities.sum(dim=2, keepdim=True)

        weights = values[:, :, None, :].expand_as(densities)
        weights = weights * densities
        weights = weights.sum(dim=3)  # I don't get this at all (sum out the MVNs)

        # Because we have 1 degree of freedom, this adds the first index of the attention mask, while the second
        # is generated by our hyper network
        out = torch.arange(context, device=util.d(x))[None, :, None, None].expand(batch, -1, num_points, 1)
        indices = torch.cat([out, indices], dim=3)

        # Here we expand the indicies for each head in this models block
        indices = indices[:, None, :, :, :].expand(-1, self.n_heads, -1, -1, -1) \
            .contiguous() \
            .view(batch * self.n_heads, context * num_points, -1)
        # Here indices has a bunch of matrices where are just lists of coordinates.
        # One matrix for each head for the whole input

        # Now expand weights (= values * densities) for each head
        weights = weights[:, None, :, :].expand(-1, self.n_heads, -1, -1) \
            .contiguous() \
            .view(batch * self.n_heads, context * num_points)

        # Perform key, query, value transformation
        K = self.to_keys(x).view(batch, context, self.n_heads, emb)
        Q = self.to_queries(x).view(batch, context, self.n_heads, emb)
        V = self.to_values(x).view(batch, context, self.n_heads, emb)

        # Because the KQV tensors have head dimension, we need to fold them back to single
        # First, transpose the head and context dimension to make it (batch, heads, context, emb)
        # Then we just get a list of matrices of size (context, emb) which would essentially be the
        # encoded sentences for each head, for each element in batch.
        K = K.transpose(1, 2).contiguous().view(batch * self.n_heads, context, emb)
        Q = Q.transpose(1, 2).contiguous().view(batch * self.n_heads, context, emb)
        V = V.transpose(1, 2).contiguous().view(batch * self.n_heads, context, emb)

        Q = Q / (emb ** (1 / 4))  # Normalize along the embedding dimension
        K = K / (emb ** (1 / 4))

        batch2, np, _ = indices.shape
        batch_is = torch.arange(batch2, dtype=torch.long, device=util.d(x))[None, :].expand(np, -1).t().reshape(-1)
        indices2 = torch.cat([batch_is[:, None], indices.view(-1, 2)], dim=-1)

        dot = util.calc_vals(Q, K.transpose(-2, -1), indices2).view(batch2, -1)
        dot = sparse.logsoftmax(indices, weights * dot, (context, context)).exp()
        out = sparse.batchmm(indices, dot, size=(context, context), xmatrix=V)  # [B * H, C, E]
        out = out.view(batch, context, self.n_heads*emb)
        return self.unify(out)


class NonadaptiveSparseAttention(_OneDimensionalSparseAttention):
    def __init__(self,
                 emb: int,
                 context_len: int,
                 *,
                 k: int = 8,
                 n_heads: int = 4,
                 gadditional: int = 2,
                 nadditional: int = 2,
                 sigma_scale: float = 1.,
                 transformation_method: str = 'sigmoid'):
        super().__init__(emb, n_heads, k=k, gadditional=gadditional, nadditional=nadditional)
        self.pmeans = torch.nn.Parameter(torch.rand((context_len, k, 1)))
        self.psigmas = torch.nn.Parameter(torch.rand((context_len, k)))
        # Non-learnabe
        self.register_buffer('pvalues', torch.ones(k))
        self.sigma_scale = sigma_scale
        self.transformation_method = transformation_method

    def hyper(self, x: torch.Tensor):
        b, c, e = x.size()
        k = self.k
        means  = self.pmeans[None, :, :, :].expand(b, c, k, 1)
        sigmas = self.psigmas[None, :, :].expand(b, c, k)
        values = self.pvalues[None, None, :].expand(b, c, k)

        means = sparse.transform_means(means, (c,), method=self.transformation_method)
        sigmas = sparse.transform_sigmas(sigmas, (c,)) * self.sigma_scale

        return means, sigmas, values


class UnknowingSparseAttention(_OneDimensionalSparseAttention):
    def __init__(self,
                 emb: int,
                 context_len: int,
                 *,
                 n_heads: int = 4,
                 gadditional: int = 2,
                 nadditional: int = 2,
                 sigma_scale: float = 1.,
                 transformation_method: str = 'modulo'):
        k = 1
        super().__init__(emb, n_heads, k=k, gadditional=gadditional, nadditional=nadditional)

        self.pmeans = torch.rand((context_len, k, 1), device='cuda')
        self.pmeans[45, 0, 0] = 112.

        self.psigmas = torch.nn.Parameter(torch.rand((context_len, k)))
        # Non-learnabe
        self.register_buffer('pvalues', torch.ones(k))
        self.sigma_scale = sigma_scale
        self.transformation_method = transformation_method

    def hyper(self, x: torch.Tensor):
        b, c, e = x.size()
        k = self.k
        means  = self.pmeans[None, :, :, :].expand(b, c, k, 1)
        sigmas = self.psigmas[None, :, :].expand(b, c, k)
        values = self.pvalues[None, None, :].expand(b, c, k)

        means = sparse.transform_means(means, (c,), method=self.transformation_method)
        sigmas = sparse.transform_sigmas(sigmas, (c,)) * self.sigma_scale
        return means, sigmas, values


class KnowingSparseAttention(_OneDimensionalSparseAttention):
    def __init__(self,
                 emb: int,
                 context_len: int,
                 *,
                 n_heads: int = 4,
                 gadditional: int = 2,
                 nadditional: int = 2,
                 sigma_scale: float = 1.,
                 transformation_method: str = 'modulo'):
        k = 1
        super().__init__(emb, n_heads, k=k, gadditional=gadditional, nadditional=nadditional)

        self.pmeans = torch.randint(100, 32000, (context_len, k, 1), device='cuda')
        for i in range(45, 105):
            self.pmeans[i, 0, 0] = i + 70

        self.psigmas = torch.nn.Parameter(torch.rand((context_len, k)))
        # Non-learnabe
        self.register_buffer('pvalues', torch.ones(k))
        self.sigma_scale = sigma_scale
        self.transformation_method = transformation_method

    def hyper(self, x: torch.Tensor):
        b, c, e = x.size()
        k = self.k
        means  = self.pmeans[None, :, :, :].expand(b, c, k, 1)
        sigmas = self.psigmas[None, :, :].expand(b, c, k)
        values = self.pvalues[None, None, :].expand(b, c, k)

        means = sparse.transform_means(means, (c,), method=self.transformation_method)
        sigmas = sparse.transform_sigmas(sigmas, (c,)) * self.sigma_scale
        return means, sigmas, values


class SparseSelfAttention(_OneDimensionalSparseAttention):

    @classmethod
    def from_config(cls, config: AdaptiveSparseAttentionConfig):
        return cls(config.emb,
                   config.context,
                   k=config.k,
                   hidden=config.hyper_hidden_dim,
                   gadditional=config.gadditional,
                   nadditional=config.nadditional)

    def __init__(self,
                 emb: int,
                 context_len: int,
                 *,
                 k: int = 8,
                 hidden: int = 4,
                 n_heads: int = 4,
                 gadditional: int = 2,
                 nadditional: int = 2):
        super().__init__(emb, n_heads, k=k, gadditional=gadditional, nadditional=nadditional)
        self.n_heads = n_heads
        self.context_len = context_len
        self.emb = emb
        self.k = k
        self.gadditional = gadditional
        self.nadditional = nadditional
        self.unify = nn.Linear(emb * n_heads, emb)
        self.register_buffer('mvalues', torch.ones((k,)))
        self.to_param = nn.Sequential(
            nn.Linear(emb, hidden),
            nn.ReLU(),
            nn.Linear(hidden, 2 * k)  # One mean and one sigma
        )

    def hyper(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        batch_size, context_len, emb = x.size()
        assert context_len == self.context_len, f'Expected contextlen equal to {self.context_len}. Got {context_len}'
        assert emb == self.emb, f'Expected embedded equal to {self.emb}. Got {emb}'

        params = self.to_param(x)  # (B, C, k*2) k means and sigmas for each point (1 degree of freedom)

        # Generate the logits that correspond to the horizontal coordinate of the current word
        diags = torch.arange(context_len, device=util.d(x), dtype=torch.float)
        diags = util.inv(diags, mx=context_len)
        diags = diags[None, :, None, None].expand(batch_size, -1, self.k, 1)  # (B, C, K, 1)

        means = params[:, :, :self.k].view(batch_size, -1, self.k, 1)  # Single mean for current point  (B, C, K, 1)
        sigmas = params[:, :, self.k:].view(batch_size, -1, self.k)  # (B, C, K)
        values = self.mvalues[None, None, :].expand(batch_size, context_len, -1)  # Expand to all points (B, C, K)

        # means = diags - torch.nn.functional.softplus(means)
        means = sparse.transform_means(means, (context_len,))
        sigmas = sparse.transform_sigmas(sigmas, (context_len,))
        return means, sigmas, values
