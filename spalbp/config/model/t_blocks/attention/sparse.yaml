attention_type: sparse
emb: ${model.embedding_dim}
heads: 4
head_size: 16
context: ${experiment.context_size}
k: 2
gadditional: 8
nadditional: 3
sigma_scale: 1.0
transformation_method: modulo
hyper_hidden_dim: 128
