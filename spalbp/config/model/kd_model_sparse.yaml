defaults:
  #- t_blocks@_t_block_dict.t_block0: base_t_block
  #- t_blocks@_t_block_dict.t_block1: base_t_block
  - tokenizer: wordpiece
  - _self_

t_blocks:
  - embedding_dim: ${model.embedding_dim}
    ff_hidden_mult: 4
    dropout: 0.0
    repeat: 8
    attention:
      type: sliding_window_with_global
  - embedding_dim: ${model.embedding_dim}
    ff_hidden_mult: 4
    dropout: 0.0
    repeat: 1
    attention:
      type: sparse
      k: 8
      gadditional: 10
      nadditional: 2
      sigma_scale: 1.0
      transformation_method: sigmoid
  - embedding_dim: ${model.embedding_dim}
    ff_hidden_mult: 4
    dropout: 0.0
    repeat: 1
    attention:
      type: sliding_window_with_global

type: transformer
embedding_dim: 64
context_size: ${experiment.context_size}
positional_encoding_type: sinusoidal
